name: Production Database Backups
on:
  schedule:
    - cron: '0 6 * * *'     # Diarios
    - cron: '0 8 1 * *'     # Mensuales
    - cron: '0 * * * *'     # Por hora
    - cron: '0 20 * * 5'    # Resumen semanal
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Tipo de backup'
        required: true
        default: 'daily'
        type: choice
        options: [daily, monthly, hourly]

env:
  NODE_VERSION: '20'

jobs:
  backup:
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install PostgreSQL 17 client
        run: |
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          echo "/usr/lib/postgresql/17/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: npm install --no-save googleapis @sendgrid/mail google-auth-library

      - name: Determine backup type
        id: backup-type
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "type=${{ github.event.inputs.backup_type }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" == "0 6 * * *" ]; then
            echo "type=daily" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" == "0 8 1 * *" ]; then
            echo "type=monthly" >> $GITHUB_OUTPUT
          else
            echo "type=hourly" >> $GITHUB_OUTPUT
          fi

      - name: Create and Run Backup Script
        env:
          BACKUP_TYPE: ${{ steps.backup-type.outputs.type }}
          DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }}
          GOOGLE_SERVICE_ACCOUNT_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
          GOOGLE_DRIVE_FOLDER_ID_DAILY: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID_DAILY }}
          GOOGLE_DRIVE_FOLDER_ID_MONTHLY: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID_MONTHLY }}
          GOOGLE_DRIVE_FOLDER_ID_HOURLY: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID_HOURLY }}
          SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}
          SENDGRID_FROM_EMAIL: ${{ secrets.SENDGRID_FROM_EMAIL }}
          BACKUP_NOTIFICATION_EMAIL: ${{ secrets.BACKUP_NOTIFICATION_EMAIL }}
        run: |
          # Aquí creamos el archivo backup.mjs al vuelo
          cat > backup.mjs << 'EOF'
          import { google } from 'googleapis';
          import { exec } from 'child_process';
          import { createReadStream, statSync, unlinkSync } from 'fs';
          import { promisify } from 'util';
          const execAsync = promisify(exec);

          const { BACKUP_TYPE, DATABASE_URL, GOOGLE_SERVICE_ACCOUNT_JSON, GOOGLE_DRIVE_FOLDER_ID_DAILY, GOOGLE_DRIVE_FOLDER_ID_MONTHLY, GOOGLE_DRIVE_FOLDER_ID_HOURLY, SENDGRID_API_KEY, NOTIFICATION_EMAIL, FROM_EMAIL } = process.env;
          const RETENTION = { daily: 30, monthly: 12, hourly: 48 };

          function getAuth() {
            return new google.auth.GoogleAuth({
              credentials: JSON.parse(GOOGLE_SERVICE_ACCOUNT_JSON),
              scopes: ['https://www.googleapis.com/auth/drive'] // SCOPE CORREGIDO
            });
          }

          async function uploadFile(filePath, fileName, folderId) {
            const drive = google.drive({ version: 'v3', auth: getAuth() });
            await drive.files.create({
              requestBody: { name: fileName, parents: [folderId] },
              media: { mimeType: 'application/gzip', body: createReadStream(filePath) },
              supportsAllDrives: true
            });
          }

          async function manageRetention(folderId, max) {
            const drive = google.drive({ version: 'v3', auth: getAuth() });
            const res = await drive.files.list({
              q: `'${folderId}' in parents and trashed=false`,
              fields: 'files(id, name)',
              orderBy: 'createdTime asc',
              supportsAllDrives: true,
              includeItemsFromAllDrives: true
            });
            const files = res.data.files || [];
            if (files.length > max) {
              for (const file of files.slice(0, files.length - max)) {
                await drive.files.delete({ fileId: file.id, supportsAllDrives: true });
              }
            }
          }

          async function run() {
            const folderId = BACKUP_TYPE === 'hourly' ? GOOGLE_DRIVE_FOLDER_ID_HOURLY : (BACKUP_TYPE === 'monthly' ? GOOGLE_DRIVE_FOLDER_ID_MONTHLY : GOOGLE_DRIVE_FOLDER_ID_DAILY);
            const fileName = `simtree-${BACKUP_TYPE}-${new Date().getTime()}.sql.gz`;
            const filePath = `/tmp/${fileName}`;
            
            console.log(`Iniciando backup: ${fileName}`);
            let cmd = `/usr/lib/postgresql/17/bin/pg_dump "${DATABASE_URL}" --no-owner --no-acl`;
            if (BACKUP_TYPE === 'hourly') cmd += ` -t wallets -t wallet_transactions -t purchased_esims`;
            
            await execAsync(`${cmd} | gzip -9 > ${filePath}`);
            await uploadFile(filePath, fileName, folderId);
            await manageRetention(folderId, RETENTION[BACKUP_TYPE]);
            unlinkSync(filePath);
            console.log("✅ Backup completado");
          }

          run().catch(async err => {
            console.error(err);
            process.exit(1);
          });
          EOF
          
          # Ahora que el archivo existe, lo ejecutamos
          node backup.mjs
